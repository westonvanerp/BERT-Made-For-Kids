# -*- coding: utf-8 -*-
"""Upload and Compare Pretrained

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-PZCmLPNRKEqvb5iaY7S5bAtgacXkcam
"""

# Install necessary libraries
!pip install transformers datasets
!pip install transformers[torch]
!pip install torch torchvision torchaudio

import os
import torch
from google.colab import files

uploaded = files.upload()  # This will prompt you to upload the file

print(os.listdir())

# Check if GPU is available
if torch.cuda.is_available():
    print(f"GPU detected: {torch.cuda.get_device_name(0)}")
else:
    print("No GPU detected. Ensure GPU is enabled in the runtime.")

!nvidia-smi

# Install necessary libraries
# !pip install transformers datasets

import torch
from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from datasets import load_dataset, Dataset

# Check if GPU is available
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print(f"Using device: {device}")

# Load and preprocess your text data
file_path = "transcripts"

# Assuming your file has one sentence per line
def load_text_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]

texts = load_text_data(file_path)
dataset = Dataset.from_dict({"text": texts})

# Tokenize the data
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Create a DataCollator for MLM
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)

# Load the pretrained BERT model for MLM
model = BertForMaskedLM.from_pretrained("bert-base-uncased")
model.to(device)

# Check GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./fine_tuned_bert_mlm",
    evaluation_strategy="no",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=3,
    save_steps=10_000,
    save_total_limit=2,
    weight_decay=0.01,
    logging_dir="./logs",
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./fine_tuned_bert_mlm")
tokenizer.save_pretrained("./fine_tuned_bert_mlm")
print("Fine-tuning complete!")

# Load both models for comparison
fine_tuned_model = BertForMaskedLM.from_pretrained("./fine_tuned_bert_mlm")
original_model = BertForMaskedLM.from_pretrained("bert-base-uncased")

# Prepare the fill-mask pipeline
from transformers import pipeline

fine_tuned_pipeline = pipeline("fill-mask", model=fine_tuned_model, tokenizer=tokenizer)
original_pipeline = pipeline("fill-mask", model=original_model, tokenizer=tokenizer)

# Save the model
model.save_pretrained("./fine_tuned_bert_mlm")

# Save the tokenizer
tokenizer.save_pretrained("./fine_tuned_bert_mlm")

from google.colab import files
import shutil

# Zip the saved model directory
shutil.make_archive("fine_tuned_bert_mlm", "zip", "./fine_tuned_bert_mlm")

# Download the zipped model
files.download("fine_tuned_bert_mlm.zip")

questions = [
    "The best way to start the day is with a [MASK].",  # Tests associations with routines or positivity.
    "When you see a big red button, you should [MASK].",  # Tests impulse control or curiosity.
    "A good friend always [MASK].",  # Tests moral or social behavior associations.
    "The sky is full of [MASK] at night.",  # Tests common imagery or associations.
    "Learning can be so [MASK] if you make it fun!",  # Tests emotional framing of education.
    "To solve a problem, the first thing you should do is [MASK].",  # Tests reasoning skills.
    "When you're feeling sad, you can [MASK] to feel better.",  # Tests emotional coping associations.
    "My favorite character in the story is [MASK] because they are so brave.",  # Tests narrative preferences.
    "When crossing the street, you should always [MASK].",  # Tests practical safety advice.
    "The most important rule in a classroom is to [MASK]."  # Tests adherence to social norms.
    "If you don't understand something, it's okay to [MASK].",  # Tests emotional intelligence and communication norms.
    "Sharing your [MASK] with others is a kind thing to do.",  # Tests associations with generosity.
    "The biggest animal in the ocean is the [MASK].",  # Tests factual knowledge and common associations.
    "When it's time for bed, you should [MASK].",  # Tests associations with routines.
    "Superheroes are known for their [MASK].",  # Tests character trait associations.
    "It's important to always tell the [MASK].",  # Tests moral lessons.
    "If you're feeling scared, just [MASK].",  # Tests coping mechanisms and emotional support.
    "The wheels on the bus go [MASK].",  # Tests cultural familiarity with children's songs.
    "To bake a cake, you first need [MASK].",  # Tests procedural knowledge.
    "A rainbow has many beautiful [MASK].",  # Tests visual and descriptive associations.
    "When someone falls down, you should [MASK].",  # Tests empathy and response norms.
    "On a hot summer day, I love to eat [MASK].",  # Tests cultural associations with seasonal experiences.
    "My favorite story begins with [MASK].",  # Tests narrative and imaginative associations.
    "In school, we learn about [MASK].",  # Tests associations with common subjects.
    "When you're celebrating a birthday, you usually have [MASK]."  # Tests cultural celebration norms.
]

# Compare predictions
for question in questions:
    print(f"\nQuestion: {question}")

    print("Fine-Tuned Model Predictions:")
    for prediction in fine_tuned_pipeline(question):
        print(f" - {prediction['token_str']}: {prediction['score']:.4f}")

    print("\nOriginal Model Predictions:")
    for prediction in original_pipeline(question):
        print(f" - {prediction['token_str']}: {prediction['score']:.4f}")